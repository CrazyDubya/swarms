from swarms.models.base_llm import BaseLLM
from typing import Optional, Dict, Any
import requests
import json
import os


class OllamaChat(BaseLLM):
    """
    A class representing an Ollama chat model for local LLM inference.

    Ollama is a tool for running large language models locally. This class provides
    an interface to interact with Ollama-served models through its REST API.

    Args:
        model_name (str): The name of the Ollama model (e.g., 'llama3.1', 'mistral', 'codellama').
        base_url (str, optional): The base URL for the Ollama API. Defaults to "http://localhost:11434".
        system_prompt (str, optional): The system prompt for the chat model. Defaults to None.
        temperature (float, optional): Temperature for sampling. Defaults to 0.7.
        max_tokens (int, optional): Maximum tokens to generate. Defaults to 2000.
        *args: Variable length argument list.
        **kwargs: Arbitrary keyword arguments.

    Attributes:
        model_name (str): The name of the Ollama model.
        base_url (str): The base URL for the Ollama API.
        system_prompt (str): The system prompt for the chat model.
        temperature (float): Temperature for sampling.
        max_tokens (int): Maximum tokens to generate.

    Methods:
        run(task, *args, **kwargs): Runs the chat model with the given task.
        __call__(task, *args, **kwargs): Alias for run method.
        list_models(): Lists available models on the Ollama server.
        is_available(): Checks if Ollama server is running and accessible.

    Example:
        >>> from swarms.models import OllamaChat
        >>> llm = OllamaChat(model_name="llama3.1")
        >>> response = llm("Hello, how are you?")
        
        # List available models
        >>> models = llm.list_models()
        >>> print(models)
        
        # Check if Ollama is available
        >>> if llm.is_available():
        ...     response = llm("What is artificial intelligence?")

    Note:
        Requires Ollama to be installed and running locally.
        Install Ollama from: https://ollama.ai/
        Start Ollama server with: ollama serve
        Pull models with: ollama pull <model_name>
    """

    def __init__(
        self,
        model_name: str,
        base_url: str = "http://localhost:11434",
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 2000,
        timeout: int = 30,
        *args,
        **kwargs,
    ):
        super().__init__(*args, **kwargs)
        self.model_name = model_name
        self.base_url = base_url.rstrip('/')
        self.system_prompt = system_prompt
        self.temperature = temperature
        self.max_tokens = max_tokens
        self.timeout = timeout

    def run(self, task: str, *args, **kwargs) -> str:
        """
        Runs the chat model with the given task.

        Args:
            task (str): The user's task for the chat model.
            *args: Variable length argument list.
            **kwargs: Arbitrary keyword arguments.

        Returns:
            str: The response generated by the chat model.

        Raises:
            RuntimeError: If Ollama server is not available or request fails.
            ValueError: If the model is not available on the server.
        """
        if not self.is_available():
            raise RuntimeError(
                f"Ollama server is not available at {self.base_url}. "
                "Please ensure Ollama is installed and running with 'ollama serve'"
            )

        # Prepare the prompt
        prompt = task
        if self.system_prompt:
            prompt = f"System: {self.system_prompt}\n\nUser: {task}"

        # Prepare request data
        data = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": kwargs.get('temperature', self.temperature),
                "num_predict": kwargs.get('max_tokens', self.max_tokens),
            }
        }

        try:
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=data,
                timeout=self.timeout,
                headers={"Content-Type": "application/json"}
            )
            response.raise_for_status()
            
            result = response.json()
            
            if 'error' in result:
                if 'not found' in result['error'].lower():
                    available_models = self.list_models()
                    raise ValueError(
                        f"Model '{self.model_name}' not found. "
                        f"Available models: {available_models}. "
                        f"Pull the model with: ollama pull {self.model_name}"
                    )
                else:
                    raise RuntimeError(f"Ollama error: {result['error']}")
            
            return result.get('response', '').strip()
            
        except requests.exceptions.Timeout:
            raise RuntimeError(f"Request to Ollama server timed out after {self.timeout} seconds")
        except requests.exceptions.ConnectionError:
            raise RuntimeError(
                f"Could not connect to Ollama server at {self.base_url}. "
                "Please ensure Ollama is running with 'ollama serve'"
            )
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Request to Ollama failed: {str(e)}")
        except json.JSONDecodeError:
            raise RuntimeError("Invalid JSON response from Ollama server")

    def __call__(self, task: str, *args, **kwargs) -> str:
        """Alias for run method to make the class callable."""
        return self.run(task, *args, **kwargs)

    def list_models(self) -> list:
        """
        Lists available models on the Ollama server.

        Returns:
            list: List of available model names.

        Raises:
            RuntimeError: If unable to fetch models from the server.
        """
        try:
            response = requests.get(
                f"{self.base_url}/api/tags",
                timeout=self.timeout
            )
            response.raise_for_status()
            
            result = response.json()
            models = [model['name'] for model in result.get('models', [])]
            return models
            
        except requests.exceptions.RequestException as e:
            raise RuntimeError(f"Failed to fetch models from Ollama: {str(e)}")
        except (json.JSONDecodeError, KeyError):
            raise RuntimeError("Invalid response format from Ollama server")

    def is_available(self) -> bool:
        """
        Checks if Ollama server is running and accessible.

        Returns:
            bool: True if Ollama server is available, False otherwise.
        """
        try:
            response = requests.get(
                f"{self.base_url}/api/version",
                timeout=5
            )
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False

    def pull_model(self, model_name: Optional[str] = None) -> bool:
        """
        Pulls/downloads a model to the Ollama server.

        Args:
            model_name (str, optional): Model name to pull. Uses self.model_name if not provided.

        Returns:
            bool: True if successful, False otherwise.

        Note:
            This is a blocking operation that may take several minutes for large models.
        """
        model = model_name or self.model_name
        
        try:
            data = {"name": model, "stream": False}
            response = requests.post(
                f"{self.base_url}/api/pull",
                json=data,
                timeout=300,  # 5 minutes timeout for model pulling
                headers={"Content-Type": "application/json"}
            )
            response.raise_for_status()
            
            result = response.json()
            return 'error' not in result
            
        except requests.exceptions.RequestException:
            return False